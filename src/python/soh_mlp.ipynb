{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multilayer Perceptron (MLP) for Battery Management System (BMS) SOH Estimation\n",
    "\n",
    "<img src=\"../../doc/img/MachineLearningNetwork.png\" height=\"1080\" width=\"1920\"\n",
    "     alt=\"Machine Learning Network\"\n",
    "     style=\"fit: left; margin-right: 10px;\"  />"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc5abf14b8daa24f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ef98a304af93377"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Flatten \n",
    "from tensorflow.keras.layers import Dense, Dropout \n",
    "from tensorflow.keras.layers import Activation \n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler  # Import MinMaxScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import MeanAbsoluteError\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T02:18:45.171351400Z",
     "start_time": "2024-02-08T02:18:45.115954500Z"
    }
   },
   "id": "eee4c6b84dd76d1e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8bbfe302bef570e"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                  V             I           SOC        T_surf           SOH\ncount  1.640946e+06  1.640946e+06  1.640946e+06  1.640946e+06  1.640946e+06\nmean   3.471013e+00  3.610140e+00  4.188879e+01  5.180549e+01  8.566437e+01\nstd    8.445745e-01  2.830966e+01  4.022232e+01  2.451698e+01  9.026790e+00\nmin   -9.394656e-01 -5.500295e+01 -5.753759e+00  2.499965e+01  7.000000e+01\n25%    3.144859e+00 -1.900000e+01  1.230206e+00  2.583835e+01  7.800000e+01\n50%    3.693897e+00  5.000000e+00  3.474387e+01  4.826449e+01  8.600000e+01\n75%    4.064538e+00  2.600000e+01  8.456010e+01  7.304868e+01  9.400000e+01\nmax    4.723157e+00  5.500289e+01  1.070423e+02  1.015903e+02  1.000000e+02",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>V</th>\n      <th>I</th>\n      <th>SOC</th>\n      <th>T_surf</th>\n      <th>SOH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.640946e+06</td>\n      <td>1.640946e+06</td>\n      <td>1.640946e+06</td>\n      <td>1.640946e+06</td>\n      <td>1.640946e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.471013e+00</td>\n      <td>3.610140e+00</td>\n      <td>4.188879e+01</td>\n      <td>5.180549e+01</td>\n      <td>8.566437e+01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>8.445745e-01</td>\n      <td>2.830966e+01</td>\n      <td>4.022232e+01</td>\n      <td>2.451698e+01</td>\n      <td>9.026790e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-9.394656e-01</td>\n      <td>-5.500295e+01</td>\n      <td>-5.753759e+00</td>\n      <td>2.499965e+01</td>\n      <td>7.000000e+01</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3.144859e+00</td>\n      <td>-1.900000e+01</td>\n      <td>1.230206e+00</td>\n      <td>2.583835e+01</td>\n      <td>7.800000e+01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.693897e+00</td>\n      <td>5.000000e+00</td>\n      <td>3.474387e+01</td>\n      <td>4.826449e+01</td>\n      <td>8.600000e+01</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.064538e+00</td>\n      <td>2.600000e+01</td>\n      <td>8.456010e+01</td>\n      <td>7.304868e+01</td>\n      <td>9.400000e+01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.723157e+00</td>\n      <td>5.500289e+01</td>\n      <td>1.070423e+02</td>\n      <td>1.015903e+02</td>\n      <td>1.000000e+02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('../../res/model_data/batemo_model_data.csv')\n",
    "df.describe()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T02:18:49.334172700Z",
     "start_time": "2024-02-08T02:18:47.966461700Z"
    }
   },
   "id": "c6d9ef80915d22ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We have to add new columns to the dataset that are the previous values of V, I and T. This will allow our model to detect the trend/gradient of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "304bf868d2b71216"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "                V          I         SOC      T_surf  SOH       V-1  \\\n1        4.198758  -0.225199  100.000000   25.000000  100  4.200000   \n2        4.197524  -0.449161  100.000000   25.000000  100  4.198758   \n23       3.984316 -39.102885  100.000000   25.000000  100  3.987069   \n24       3.982531 -39.423824  100.000000   25.000000  100  3.984316   \n26       3.980624 -39.764270  100.000000   25.000000  100  3.981373   \n...           ...        ...         ...         ...  ...       ...   \n1640941  3.040704 -41.000000   14.330671   95.558472   70  3.060165   \n1640942  3.019722 -41.000000   13.112293   96.489709   70  3.040704   \n1640943  2.985720 -41.000000   11.355930   97.840139   70  3.019722   \n1640944  2.945987 -41.000000    9.599568   99.207108   70  2.985720   \n1640945  2.899612 -41.000000    7.843205  100.599081   70  2.945987   \n\n               I-1   T_surf-1  \n1         0.000000  25.000000  \n2        -0.225199  25.000000  \n23      -38.606253  25.000000  \n24      -39.102885  25.000000  \n26      -39.631206  25.000000  \n...            ...        ...  \n1640941 -41.000000  94.628477  \n1640942 -41.000000  95.558472  \n1640943 -41.000000  96.489709  \n1640944 -41.000000  97.840139  \n1640945 -41.000000  99.207108  \n\n[1637505 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>V</th>\n      <th>I</th>\n      <th>SOC</th>\n      <th>T_surf</th>\n      <th>SOH</th>\n      <th>V-1</th>\n      <th>I-1</th>\n      <th>T_surf-1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>4.198758</td>\n      <td>-0.225199</td>\n      <td>100.000000</td>\n      <td>25.000000</td>\n      <td>100</td>\n      <td>4.200000</td>\n      <td>0.000000</td>\n      <td>25.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.197524</td>\n      <td>-0.449161</td>\n      <td>100.000000</td>\n      <td>25.000000</td>\n      <td>100</td>\n      <td>4.198758</td>\n      <td>-0.225199</td>\n      <td>25.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>3.984316</td>\n      <td>-39.102885</td>\n      <td>100.000000</td>\n      <td>25.000000</td>\n      <td>100</td>\n      <td>3.987069</td>\n      <td>-38.606253</td>\n      <td>25.000000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>3.982531</td>\n      <td>-39.423824</td>\n      <td>100.000000</td>\n      <td>25.000000</td>\n      <td>100</td>\n      <td>3.984316</td>\n      <td>-39.102885</td>\n      <td>25.000000</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>3.980624</td>\n      <td>-39.764270</td>\n      <td>100.000000</td>\n      <td>25.000000</td>\n      <td>100</td>\n      <td>3.981373</td>\n      <td>-39.631206</td>\n      <td>25.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1640941</th>\n      <td>3.040704</td>\n      <td>-41.000000</td>\n      <td>14.330671</td>\n      <td>95.558472</td>\n      <td>70</td>\n      <td>3.060165</td>\n      <td>-41.000000</td>\n      <td>94.628477</td>\n    </tr>\n    <tr>\n      <th>1640942</th>\n      <td>3.019722</td>\n      <td>-41.000000</td>\n      <td>13.112293</td>\n      <td>96.489709</td>\n      <td>70</td>\n      <td>3.040704</td>\n      <td>-41.000000</td>\n      <td>95.558472</td>\n    </tr>\n    <tr>\n      <th>1640943</th>\n      <td>2.985720</td>\n      <td>-41.000000</td>\n      <td>11.355930</td>\n      <td>97.840139</td>\n      <td>70</td>\n      <td>3.019722</td>\n      <td>-41.000000</td>\n      <td>96.489709</td>\n    </tr>\n    <tr>\n      <th>1640944</th>\n      <td>2.945987</td>\n      <td>-41.000000</td>\n      <td>9.599568</td>\n      <td>99.207108</td>\n      <td>70</td>\n      <td>2.985720</td>\n      <td>-41.000000</td>\n      <td>97.840139</td>\n    </tr>\n    <tr>\n      <th>1640945</th>\n      <td>2.899612</td>\n      <td>-41.000000</td>\n      <td>7.843205</td>\n      <td>100.599081</td>\n      <td>70</td>\n      <td>2.945987</td>\n      <td>-41.000000</td>\n      <td>99.207108</td>\n    </tr>\n  </tbody>\n</table>\n<p>1637505 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "# Round the I column for simplicity\n",
    "df['I_round'] = df['I'].round(0)\n",
    "\n",
    "# Define the columns to shift\n",
    "cols_to_shift = ['V', 'I', 'T_surf']\n",
    "\n",
    "# Apply the shift to the entire dataframe, but only on each unique SOH, I Pair\n",
    "for col in cols_to_shift:\n",
    "    df[col + '-1'] = df.groupby(['SOH', 'I_round'])[col].shift(1)\n",
    "\n",
    "# Drop rows with NaN values and I_round column\n",
    "df = df.dropna()\n",
    "df = df.drop(columns=['I_round'])\n",
    "\n",
    "# Define training variables\n",
    "X = df[['V', 'I', 'T_surf', 'V-1', 'I-1', 'T_surf-1']]\n",
    "Y = df[['SOH']]\n",
    "\n",
    "# Normalize the input data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T02:18:52.406898100Z",
     "start_time": "2024-02-08T02:18:51.081412300Z"
    }
   },
   "id": "8f6b400964bd537f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbf475749b0d41a5"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Model Training with regularization and dropout\n",
    "# model = Sequential([\n",
    "#     # input layer\n",
    "#     Input(shape=(6, )),\n",
    "#\n",
    "#     # dense layer 1 with L1 regularization\n",
    "#     Dense(256, activation='sigmoid', kernel_regularizer=l1(0.01)),\n",
    "#\n",
    "#     # dropout layer\n",
    "#     Dropout(0.5),\n",
    "#\n",
    "#     # dense layer 2 with L2 regularization\n",
    "#     Dense(128, activation='sigmoid', kernel_regularizer=l2(0.01)),\n",
    "#\n",
    "#     # dropout layer\n",
    "#     Dropout(0.5),\n",
    "#\n",
    "#     # output layer\n",
    "# #     Dense([70, 100], activation='relu'),\n",
    "# # ])\n",
    "#\n",
    "#     # output layer with linear activation\n",
    "#     Dense(1, activation='linear'),\n",
    "# ])\n",
    "\n",
    "# Define the architecture of the MLP model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(6,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T02:18:54.265629800Z",
     "start_time": "2024-02-08T02:18:54.102854100Z"
    }
   },
   "id": "6c99a738a888d283"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OLD Model Testing (commented out code)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6261252fccf2cd5c"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Model Testing\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.compile(optimizer=Adam(learning_rate=0.3),\n",
    "#               loss='mean_squared_error',  # Use mean squared error for regression\n",
    "#               metrics=['mse'])\n",
    "\n",
    "# define training variables V, I, and T here: \n",
    "# X = df[['V', 'I', 'T_surf', 'V-1', 'I-1', 'T_surf-1']]\n",
    "# Y = df[['SOH']]\n",
    "\n",
    "# Scale the output variable to [0, 1]\n",
    "# scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "# Y_scaled = scaler.fit_transform(Y)\n",
    "\n",
    "# Normalize the input data\n",
    "# scaler = MinMaxScaler()\n",
    "# X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# split up the data into training and testing \n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# model.fit(x_train, y_train, epochs=10,\n",
    "# \t\tbatch_size=2000,\n",
    "# \t\tvalidation_split=0.2)\n",
    "#\n",
    "# results = model.evaluate(x_test, y_test, verbose = 0)\n",
    "# print('test loss, test mse:', results)\n",
    "\n",
    "\n",
    "# # Define a custom learning rate\n",
    "# custom_learning_rate = 0.001  # Adjust this value as needed\n",
    "# \n",
    "# # Create an instance of the Adam optimizer with the custom learning rate\n",
    "# custom_adam_optimizer = Adam(learning_rate=custom_learning_rate)\n",
    "# \n",
    "# # Compile the model with the custom Adam optimizer\n",
    "# model.compile(optimizer=custom_adam_optimizer, loss='mean_squared_error')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T02:18:59.907753Z",
     "start_time": "2024-02-08T02:18:59.892130800Z"
    }
   },
   "id": "4fda4d1f7b42b329"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New Model Testing using Adam as optimizier, MSE and MAE to measure performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd25478c46812473"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "36844/36844 [==============================] - 58s 2ms/step - loss: 62.7960 - val_loss: 38.4160\n",
      "Epoch 2/50\n",
      "36844/36844 [==============================] - 59s 2ms/step - loss: 40.6630 - val_loss: 36.4122\n",
      "Epoch 3/50\n",
      "36844/36844 [==============================] - 60s 2ms/step - loss: 38.1702 - val_loss: 35.2494\n",
      "Epoch 4/50\n",
      "36844/36844 [==============================] - 62s 2ms/step - loss: 37.0476 - val_loss: 37.2267\n",
      "Epoch 5/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 36.5136 - val_loss: 35.1959\n",
      "Epoch 6/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 36.2103 - val_loss: 35.9959\n",
      "Epoch 7/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 35.9092 - val_loss: 35.8242\n",
      "Epoch 8/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 35.6906 - val_loss: 35.2146\n",
      "Epoch 9/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 35.6533 - val_loss: 35.9705\n",
      "Epoch 10/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 35.2150 - val_loss: 33.7991\n",
      "Epoch 11/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 35.1211 - val_loss: 33.6457\n",
      "Epoch 12/50\n",
      "36844/36844 [==============================] - 63s 2ms/step - loss: 34.9462 - val_loss: 36.0779\n",
      "Epoch 13/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 34.8862 - val_loss: 33.0446\n",
      "Epoch 14/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 34.8513 - val_loss: 33.1003\n",
      "Epoch 15/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 34.7920 - val_loss: 36.0576\n",
      "Epoch 16/50\n",
      "36844/36844 [==============================] - 64s 2ms/step - loss: 34.6060 - val_loss: 32.4710\n",
      "Epoch 17/50\n",
      "36844/36844 [==============================] - 70s 2ms/step - loss: 34.4153 - val_loss: 34.0350\n",
      "Epoch 18/50\n",
      "36844/36844 [==============================] - 70s 2ms/step - loss: 34.3559 - val_loss: 34.3651\n",
      "Epoch 19/50\n",
      "36844/36844 [==============================] - 69s 2ms/step - loss: 34.2379 - val_loss: 32.8357\n",
      "Epoch 20/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 34.2090 - val_loss: 34.0931\n",
      "Epoch 21/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 34.1775 - val_loss: 32.7206\n",
      "Epoch 22/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 34.2222 - val_loss: 32.0809\n",
      "Epoch 23/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 33.9986 - val_loss: 34.3021\n",
      "Epoch 24/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 33.8790 - val_loss: 36.9732\n",
      "Epoch 25/50\n",
      "36844/36844 [==============================] - 68s 2ms/step - loss: 33.6986 - val_loss: 35.2551\n",
      "Epoch 26/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 33.6414 - val_loss: 31.6123\n",
      "Epoch 27/50\n",
      "36844/36844 [==============================] - 68s 2ms/step - loss: 33.4551 - val_loss: 30.9141\n",
      "Epoch 28/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 33.2377 - val_loss: 30.4220\n",
      "Epoch 29/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 33.1388 - val_loss: 33.5704\n",
      "Epoch 30/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 32.9594 - val_loss: 30.6256\n",
      "Epoch 31/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 32.8684 - val_loss: 32.5131\n",
      "Epoch 32/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 32.8855 - val_loss: 31.2631\n",
      "Epoch 33/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 32.7293 - val_loss: 32.9048\n",
      "Epoch 34/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 32.6794 - val_loss: 30.8280\n",
      "Epoch 35/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 32.4748 - val_loss: 31.4971\n",
      "Epoch 36/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 32.3718 - val_loss: 37.7823\n",
      "Epoch 37/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 31.9792 - val_loss: 33.4733\n",
      "Epoch 38/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 31.8658 - val_loss: 29.8584\n",
      "Epoch 39/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 31.8583 - val_loss: 29.9399\n",
      "Epoch 40/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 31.8841 - val_loss: 31.1101\n",
      "Epoch 41/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 31.5648 - val_loss: 29.8189\n",
      "Epoch 42/50\n",
      "36844/36844 [==============================] - 68s 2ms/step - loss: 31.3562 - val_loss: 31.6280\n",
      "Epoch 43/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 31.2469 - val_loss: 29.7260\n",
      "Epoch 44/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 31.1084 - val_loss: 30.2324\n",
      "Epoch 45/50\n",
      "36844/36844 [==============================] - 67s 2ms/step - loss: 31.1120 - val_loss: 29.9134\n",
      "Epoch 46/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 31.1401 - val_loss: 33.2566\n",
      "Epoch 47/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 31.1218 - val_loss: 37.9665\n",
      "Epoch 48/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 31.1061 - val_loss: 31.4985\n",
      "Epoch 49/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 31.1222 - val_loss: 31.3830\n",
      "Epoch 50/50\n",
      "36844/36844 [==============================] - 66s 2ms/step - loss: 31.0399 - val_loss: 29.6312\n",
      "10235/10235 [==============================] - 18s 2ms/step - loss: 29.5336\n",
      "Test Loss (Adam): 29.533565521240234\n",
      "10235/10235 [==============================] - 15s 1ms/step - loss: 3.9646\n",
      "MAE (Adam): 3.964630603790283\n"
     ]
    }
   ],
   "source": [
    "# Define custom learning rates\n",
    "adam_learning_rate = 0.03 # default learning rate for Adam in Keras is 0.001\n",
    "\n",
    "# Compile the model with Adam optimizer and custom learning rate\n",
    "adam_optimizer = Adam(learning_rate=adam_learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "# model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "model.compile(optimizer=adam_optimizer, loss='mean_squared_error') # use custom learning rate to see if this improves performance \n",
    "# we are using Adam as the optimizer as opposed to using SGD \n",
    "# Adam typically offers faster convergence and better performance but requires tuning its hyperparameters\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(x_train, y_train, batch_size=32, epochs=50, validation_split=0.1)\n",
    "history_adam = model.fit(x_train, y_train, batch_size=32, epochs=50, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "# loss = model.evaluate(x_test, y_test)\n",
    "# print(\"Test Loss:\", loss)\n",
    "loss_adam = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss (Adam):\", loss_adam)\n",
    "\n",
    "# Compile the model with MAE loss for Adam optimizer\n",
    "model.compile(optimizer=Adam(), loss=MeanAbsoluteError())\n",
    "\n",
    "# Evaluate the model using MAE for Adam optimizer\n",
    "mae_adam = model.evaluate(x_test, y_test)\n",
    "print(\"MAE (Adam):\", mae_adam)\n",
    "\n",
    "# TODO: add additional metrics such as MAE to make sure the model will perform fine on unseen data \n",
    "# Compile the model with MAE loss\n",
    "# model.compile(optimizer=Adam(), loss=MeanAbsoluteError())\n",
    "# \n",
    "# # Evaluate the model using MAE\n",
    "# mae = model.evaluate(x_test, y_test)\n",
    "# print(\"MAE:\", mae)\n",
    "# \n",
    "# model_using_SGD.compile(optimizer=sgd_optimizer, loss=MeanAbsoluteError())\n",
    "# \n",
    "# # Evaluate the model using MAE\n",
    "# mae = model_using_SGD.evaluate(x_test, y_test)\n",
    "# print(\"MAE:\", mae)\n",
    "\n",
    "\n",
    "# NOTE: after re-running the model multiple times, the accuracy seems to \"improve\" \n",
    "# this is due to the learned weights for theta after each run, making the model seem more accurate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T03:14:22.506377100Z",
     "start_time": "2024-02-08T02:19:03.255957500Z"
    }
   },
   "id": "1d27d9f033551b06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add a model that uses SGD as the optimizier to compare performance with the Adam optimizier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccd44964b65ebf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add second model using SGD as the optimizer to see if one out-performs the other \n",
    "sgd_learning_rate = 0.03 # default learning rate for SGD is 0.01\n",
    "\n",
    "# Custom SGD optimizer with learning rate of 0.01 and momentum of 0.9\n",
    "# sgd_optimizer = SGD(lr=0.01, momentum=0.9)\n",
    "# \n",
    "# # Compile the model using the custom SGD optimizer\n",
    "# model_using_SGD.compile(optimizer=sgd_optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Compile the model with SGD optimizer and custom learning rate\n",
    "sgd_optimizer = SGD(lr=sgd_learning_rate, momentum=0.9)\n",
    "model.compile(optimizer=sgd_optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Train the model with SGD optimizer\n",
    "history_sgd = model.fit(x_train, y_train, batch_size=32, epochs=50, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model trained with SGD optimizer\n",
    "loss_sgd = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss (SGD):\", loss_sgd)\n",
    "\n",
    "# Compile the model with MAE loss for SGD optimizer\n",
    "model.compile(optimizer=sgd_optimizer, loss=MeanAbsoluteError())\n",
    "\n",
    "# Evaluate the model using MAE for SGD optimizer\n",
    "mae_sgd = model.evaluate(x_test, y_test)\n",
    "print(\"MAE (SGD):\", mae_sgd)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dfaded5a0f5fdfe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Export"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e9bdcdbfba190be"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Model Export\n",
    "# save Keras model\n",
    "model.save(\"model_file_name\" +'.h5')\n",
    "# convert Keras model to a tflite model \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "with open(\"model_file_name\" + '.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-07T01:53:56.054193600Z",
     "start_time": "2024-02-07T01:53:56.036672Z"
    }
   },
   "id": "292d37d85f804733"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
