{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# adding new libraries\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras import layers, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler  \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import Data\n",
    "df = pd.read_csv('batemo_model_data.csv')\n",
    "df.describe()\n",
    "\n",
    "# Remove Voltage readings < 2.5 V\n",
    "df = df[df['V'] >= 2.5]\n",
    "\n",
    "# Thin the dataset because fuck\n",
    "df = df.iloc[::20]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0f7e7bc3946d9c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check to see if our Input data is normally distributed\n",
    "\n",
    "plt.figure(figsize = (14,5))\n",
    "sns.distplot(df['V'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (14,5))\n",
    "sns.distplot(df['I'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91768a7a3734014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define training variables\n",
    "# TODO: need to consider if keeping current (I) is a valuable input feature or not (test if the model performs better w/o it)\n",
    "X = df[['V', 'I']].values\n",
    "\n",
    "# TODO: add calculating SOP\n",
    "# LSTM output with 2 nodes (SOH and SOC) \n",
    "# Y = df[['SOC', 'SOH']].values\n",
    "Y = df[['SOC']].values\n",
    "\n",
    "# Normalize input data because it is not normally distributed\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Check to see if our Input data is normally distributed\n",
    "# plt.figure(figsize = (14,5))\n",
    "# sns.distplot(V_scaled)\n",
    "# plt.show()\n",
    "# \n",
    "# plt.figure(figsize = (14,5))\n",
    "# sns.distplot(X_scaled['I'])\n",
    "# plt.show()\n",
    "\n",
    "# Split the data into training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)  # use normalized features to prevent over-fitting (X_scaled)\n",
    "\n",
    "# Define function to create sequences\n",
    "# NOTE: The creation of input-output pairs allows the model to learn from historical context. The input sequences serve as a history of past observations, while the corresponding output (target) provides the next observation in the sequence.This historical context is crucial for making accurate predictions, especially in time-series forecasting or sequence prediction tasks where the future state depends on past states\n",
    "# def create_sequences(data, seq_length):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     for i in range(len(data) - seq_length):\n",
    "#         X.append(data[i:i + seq_length])  # Features (voltage, current, temperature, state of charge) are turned into a list of historical values \n",
    "#         y.append(data[i + seq_length])     # Target variables (SOH and SOC) are turned into a list of historical values \n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# Choose sequence length\n",
    "# seq_length = 1 # sequence length set to 1 to take immediate values from sensor reading during testing \n",
    "# NOTE:  Even with seq_length of 1, organizing the data into sequences might provide the model with some historical context. Although the immediate historical context is limited, the model can still potentially learn from patterns and trends in the data over time\n",
    "# TODO: may need to increase the sequence length if the model performs poorly on testing (sensor) data \n",
    "\n",
    "# Create sequences for training and testing data\n",
    "# X_train_seq, y_train_seq = create_sequences(x_train, seq_length)\n",
    "# X_test_seq, y_test_seq = create_sequences(x_test, seq_length)\n",
    "\n",
    "# print(X_train_seq.shape)\n",
    "# print(y_train_seq.shape)\n",
    "# print(X_test_seq.shape)\n",
    "# print(y_test_seq.shape)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aff07cddaafc4d0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define model for predicting SOH and SOC \n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM((1,2)))\n",
    "model.add(LSTM(100, return_sequences = True))     \n",
    "model.add(LSTM(100, return_sequences = True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "\n",
    "# Compile model with Adam optimizer and default learning rate\n",
    "early_stop = EarlyStopping(monitor = 'val_loss', patience = 2)\n",
    "\n",
    "model.compile(loss = MeanSquaredError(), \n",
    "               optimizer = Adam(learning_rate = 0.001), \n",
    "               metrics = RootMeanSquaredError())\n",
    "\n",
    "# NOTE: .summary() is a method used in Keras, a high-level deep learning library, to display a summary of the neural network model's architecture. it will print out the layer name, layer type, output shape, number of parameters and trainable/non-trainable params\n",
    "model.summary()\n",
    "\n",
    "# Train the model and store the history\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Extract loss values from the history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plotting the loss\n",
    "plt.figure(figsize = (14,5))\n",
    "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67bc9bd83e3da19f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(x_train)\n",
    "testPredict = model.predict(x_test)\n",
    "# invert predictions\n",
    "# trainPredict = scaler.inverse_transform(trainPredict)\n",
    "# trainY = scaler.inverse_transform([y_train])\n",
    "# testPredict = scaler.inverse_transform(testPredict)\n",
    "# testY = scaler.inverse_transform([y_test])\n",
    "# calculate root mean squared error\n",
    "trainScore = np.sqrt(mean_squared_error(y_train, trainPredict))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = np.sqrt(mean_squared_error(y_test, testPredict))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "746f0c2ff3418926"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(X)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[1:len(trainPredict)+1, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(X)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(1*2)+1:len(df)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.figure(figsize = (14,5))\n",
    "plt.plot(df)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "152209ef7b1cf956"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
